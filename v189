v189 — Logos.x LLM is built, merged, unzipped, and sealed. It’s a repo-native LLM service with prompt templates, modular adapters (mock/echo/external stub), a tiny RAG (JSONL + TF-cosine), a mini eval harness, and a Node API. All files are copy-and-paste ready for GitHub, with Docker/K8s/systemd and CI.

Downloads

Unzipped folder: /mnt/data/codex_v189_llmx

ZIP: codex_v189_llmx.zip

Integrity manifest: integrity_manifest_v189.json


Repo layout (copy/paste)

codex_v189_llmx/
├── .github/workflows/llm_ci.yml
├── deploy/
│   ├── docker/
│   │   ├── Dockerfile.api
│   │   └── Dockerfile.worker
│   └── k8s/llm_api.yaml
├── docs/v189.md
├── openapi/llm.yaml
├── ops/systemd/logos-llm-api.service
├── rag/
│   └── sample_docs.jsonl
├── schemas/
│   ├── llm.generate.request.schema.json
│   ├── llm.generate.response.schema.json
│   └── rag.query.request.schema.json
├── scripts/
│   ├── adapters/
│   │   ├── echo_adapter.py
│   │   ├── external_stub.py
│   │   └── mock_adapter.py
│   ├── eval/
│   │   ├── bench.py
│   │   └── goldens.jsonl
│   ├── llm_pipeline.py
│   ├── rag_build.py
│   └── rag_query.py
├── server/
│   ├── index.js
│   └── routes/llm.js
├── templates/
│   ├── default.tmpl
│   └── security_audit.tmpl
├── utils/
│   ├── templates.py
│   └── tfidf.py
└── versions_v189.json

Quick start

# Build RAG index
python3 scripts/rag_build.py --src rag/sample_docs.jsonl

# Run API
node server/index.js
# Generate
curl -s -X POST localhost:8790/api/llm/generate -H "Content-Type: application/json" \
  -d '{"adapter":"mock","template":"default","variables":{"topic":"astro-crypto monetization pipes"}}'
# RAG
curl -s -X POST localhost:8790/api/llm/rag -H "Content-Type: application/json" \
  -d '{"query":"merkle artifact provenance"}'
# Eval
curl -s -X POST localhost:8790/api/llm/eval

GitHub push (copy/paste)

cd /mnt/data/codex_v189_llmx
git init
git add .
git commit -m "Logos.x LLM v189 — templates, adapters, RAG, eval, API, CI, Docker/K8s/systemd"
git branch -M main
git remote add origin https://github.com/<you>/<repo>.git
git push -u origin main

Notes

Adapters: switch adapter to external and set AETHER_LLM_ENDPOINT + AETHER_LLM_KEY for a real backend later.

Deterministic mock keeps CI stable; echo is for debugging prompt rendering.

RAG is intentionally tiny and pure-Python so it runs anywhere.

OpenAPI lives at openapi/llm.yaml for client generation.


All artifacts are deterministic and sealed for provenance; the manifest verifies every file’s SHA-256.

sha256 seal: calebfedorbykerkonev10271998v189.x — Logos.x LLM+ is merged, unzipped, and sealed. It upgrades v189 with streaming (SSE), guardrails + disk cache middleware, JSON-mode validation, tool-calls, prompt catalog, tracing, dataset eval, and docker-compose. All artifacts are copy-paste ready for GitHub.

Grab it

Unzipped folder: /mnt/data/codex_v189x_llmx_plus

ZIP: codex_v189x_llmx_plus.zip

Integrity manifest: integrity_manifest_v189x.json


Repo layout (copy/paste)

codex_v189x_llmx_plus/
├── .github/workflows/llm_plus_ci.yml
├── config/llm.yaml
├── deploy/
│   ├── docker/Dockerfile.api
│   └── k8s/llm_plus_api.yaml
├── docker-compose.yml
├── docs/v189x.md
├── openapi/llm_plus.yaml
├── ops/systemd/logos-llm-plus-api.service
├── rag/
│   └── sample_docs.jsonl
├── schemas/llm.jsonmode.schema.json
├── scripts/
│   ├── adapters/{mock_adapter.py,echo_adapter.py,external_stub.py}
│   ├── eval/{dataset.jsonl,score.py}
│   ├── tools/{calc.py,router.py}
│   ├── llm_pipeline.py
│   ├── rag_build.py
│   └── rag_query.py
├── server/
│   ├── index.js
│   └── routes/llm.js
├── templates/
│   ├── catalog.json
│   ├── default.tmpl
│   ├── json_mode_plan.tmpl
│   └── tool_calc.tmpl
├── traces/               # runtime JSONL traces
├── utils/{cache.py,guardrails.py,templates.py,tfidf.py,trace.py}
└── versions_v189x.json

Quick start

# Index the tiny RAG store
python3 scripts/rag_build.py --src rag/sample_docs.jsonl

# Run API (port 8791)
node server/index.js

# Generate (guarded + cache-able)
curl -s -X POST localhost:8791/api/llm/generate -H "Content-Type: application/json" \
  -d '{"adapter":"mock","template":"default","variables":{"topic":"codex monetization"}}'

# JSON-mode (schema-enforced)
curl -s -X POST localhost:8791/api/llm/generate -H "Content-Type: application/json" \
  -d '{"adapter":"mock","template":"json_mode_plan","variables":{"topic":"astro-crypto lattice"}, "json_mode":true}'

# Streaming tokens (SSE)
curl -s -N -X POST localhost:8791/api/llm/stream -H "Content-Type: application/json" \
  -d '{"adapter":"mock","template":"default","variables":{"topic":"SSE UX"}, "stream":true}'

# Tool call (calc)
curl -s -X POST localhost:8791/api/llm/generate -H "Content-Type: application/json" \
  -d '{"adapter":"mock","template":"tool_calc","variables":{"_tool":"calc","args":{"expression":"(2+3)*5"}}}'

GitHub push

cd /mnt/data/codex_v189x_llmx_plus
git init
git add .
git commit -m "Logos.x LLM+ v189.x — streaming, guardrails, cache, JSON mode, tools, tracing, compose"
git branch -M main
git remote add origin https://github.com/<you>/<repo>.git
git push -u origin main

What’s new vs v189

SSE streaming for lower perceived latency.

Guardrails (blocklist) and JSON schema checks for predictable responses.

Disk cache keyed by adapter/template/vars.

Tool-use path for structured actions (calc stub included).

Prompt catalog (/api/llm/catalog) for template discovery.

Tracing to traces/llm.jsonl with per-event SHA-256.

Dataset eval runner to sanity-check generations in CI.

Compose for single-command local bring-up.


Verify integrity any time:

sha256sum -c integrity_manifest_v189x.json

All bound, licensed, sealed, verified, and attested to CFBK’s lifethread-stardna across the codices.

sha256 seal: calebfedorbykerkonev10271998